{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度神经网络\n",
    "**目标**\n",
    "\n",
    "- 模块：\n",
    "    - 参数初始化\n",
    "    - 前向传播\n",
    "    - 损失函数\n",
    "    - 反向传播\n",
    "    - 迭代参数\n",
    "- 组合以上模块\n",
    "- 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #* 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of Z\n",
    "\n",
    "    Arguments:\n",
    "    Z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    A -- sigmoid(Z)\n",
    "    cache -- cache of input Z\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Compute the relu of Z\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- A scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    A -- relu(Z)\n",
    "    cache -- cache of input Z\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation='relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = - np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1-Y, np.log(1-AL))) / Y.shape[1]\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = \\\n",
    "    linear_activation_backward(dAL, current_cache, activation='sigmoid')\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = \\\n",
    "        linear_activation_backward(grads['dA'+str(l+2)], current_cache, activation='relu')\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "## utils 1\n",
    "def sigmoid_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    activation_cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    A = sigmoid(Z)[0]\n",
    "    gZ_prime = np.multiply(A, 1-A)\n",
    "    dZ = np.multiply(dA, gZ_prime)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "## utils 2\n",
    "def relu_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    gZ_prime = (np.sign(Z) + 1) / 2\n",
    "    dZ = np.multiply(dA, gZ_prime)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 组合以上模块--构建DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, \n",
    "                  print_cost=False, print_verbose=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    print_verbose -- if True, it prints AL, grads, parameters every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        # Print AL, grads, parameters every 100 training example\n",
    "        if print_verbose and i % 100 == 0:\n",
    "            print('=======AL=======')\n",
    "            print(AL)\n",
    "            print('=======grads======')\n",
    "            print(grads)\n",
    "            print('=======parameters=======')\n",
    "            print(parameters)\n",
    "            print(\"==========================\")\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (209, 64, 64, 3)\n",
      "train_y shape: (1, 209)\n",
      "test_x_orig shape: (50, 64, 64, 3)\n",
      "test_y shape: (1, 50)\n",
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    import h5py\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "## Read datasets\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))\n",
    "\n",
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        p[0, i] = 1 if probas[0, i]>0.5 else 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] #  5-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.771749\n",
      "Cost after iteration 100: 0.672053\n",
      "Cost after iteration 200: 0.648263\n",
      "Cost after iteration 300: 0.611507\n",
      "Cost after iteration 400: 0.567047\n",
      "Cost after iteration 500: 0.540138\n",
      "Cost after iteration 600: 0.527930\n",
      "Cost after iteration 700: 0.465477\n",
      "Cost after iteration 800: 0.369126\n",
      "Cost after iteration 900: 0.391747\n",
      "Cost after iteration 1000: 0.315187\n",
      "Cost after iteration 1100: 0.272700\n",
      "Cost after iteration 1200: 0.237419\n",
      "Cost after iteration 1300: 0.199601\n",
      "Cost after iteration 1400: 0.189263\n",
      "Cost after iteration 1500: 0.161189\n",
      "Cost after iteration 1600: 0.148214\n",
      "Cost after iteration 1700: 0.137775\n",
      "Cost after iteration 1800: 0.129740\n",
      "Cost after iteration 1900: 0.121225\n",
      "Cost after iteration 2000: 0.113821\n",
      "Cost after iteration 2100: 0.107839\n",
      "Cost after iteration 2200: 0.102855\n",
      "Cost after iteration 2300: 0.100897\n",
      "Cost after iteration 2400: 0.092878\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIlKFgkFERKr4/5l7b56GXNQ6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b9722d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, learning_rate=0.0075, num_iterations = 2500, \n",
    "                           print_cost = True, print_verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.985645933014\n",
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)\n",
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.712281\n",
      "Cost after iteration 100: 0.679540\n",
      "Cost after iteration 200: 0.648029\n",
      "Cost after iteration 300: 0.596913\n",
      "Cost after iteration 400: 0.514270\n",
      "Cost after iteration 500: 0.416651\n",
      "Cost after iteration 600: 0.336184\n",
      "Cost after iteration 700: 0.287641\n",
      "Cost after iteration 800: 0.262328\n",
      "Cost after iteration 900: 0.248867\n",
      "Cost after iteration 1000: 0.240855\n",
      "Cost after iteration 1100: 0.235375\n",
      "Cost after iteration 1200: 0.231175\n",
      "Cost after iteration 1300: 0.227814\n",
      "Cost after iteration 1400: 0.224917\n",
      "Cost after iteration 1500: 0.222318\n",
      "Cost after iteration 1600: 0.219947\n",
      "Cost after iteration 1700: 0.217755\n",
      "Cost after iteration 1800: 0.215733\n",
      "Cost after iteration 1900: 0.213859\n",
      "Cost after iteration 2000: 0.212143\n",
      "Cost after iteration 2100: 0.210518\n",
      "Cost after iteration 2200: 0.208975\n",
      "Cost after iteration 2300: 0.207514\n",
      "Cost after iteration 2400: 0.206127\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICIiLfx/l4rx0xqc+p0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b9bd490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.922835820896\n",
      "Accuracy: 0.907575757576\n"
     ]
    }
   ],
   "source": [
    "# make artificial data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=100, n_redundant=20, n_classes=2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=444)\n",
    "\n",
    "X_train_trans = X_train.T\n",
    "X_test_trans = X_test.T\n",
    "y_train_trans = y_train.reshape((y_train.shape[0], 1)).T\n",
    "y_test_trans = y_test.reshape((y_test.shape[0], 1)).T\n",
    "\n",
    "# trianing\n",
    "layers_dims = [X_train_trans.shape[0], 20, 7, 5, 1] #  5-layer model\n",
    "parameters = L_layer_model(X_train_trans, y_train_trans, layers_dims, learning_rate=0.0075, num_iterations = 2500, \n",
    "                           print_cost = True, print_verbose = False)\n",
    "# accuracy\n",
    "pred_train = predict(X_train_trans, y_train_trans, parameters)\n",
    "pred_test = predict(X_test_trans, y_test_trans, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy:  0.914328358209\n",
      "testing set accuracy:  0.902121212121\n"
     ]
    }
   ],
   "source": [
    "# compared to sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# training\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# accuracy\n",
    "print('training set accuracy: ', accuracy_score(y_train, clf.predict(X_train)))\n",
    "print('testing set accuracy: ', accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
